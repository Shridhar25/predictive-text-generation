PREDICTIVE TEXT GENERATION

Predicting the next word has been one of the most important subjects of discussion in Natural Language Processing. Nowadays, instead of wasting time on writing everything and then proof reading it, we simply use Auto-complete. We use it every day but donâ€™t give it time of day to understand how it happens. Natural language generation (NLG) focuses on generation of natural, human interpretable language. NLG is a methodology that allows us to predict the next word in a sentence most likely to be used.
Sequence prediction problems have been a major problem for a long time. Recurrent Neural Network (RNN) has been a good solution for sequential prediction problems. This work aims to create a generative model for text. Even though, RNN has its own limitations such as vanishing and exploding gradient descent problems and inefficiency to keep track of long-term dependencies. To overcome these drawbacks, Long Short Term Memory (LSTM) has been a path-breaking solution to deal with sequential data and text data in particular.
The project aims to develop a model that predicts words based on users typing and suggests users some words that can possibly come next. The LSTM model and BiLSTM model comparative analysis is done to study how the model behaves differently. The project also aims to review all the studies and efforts made by researchers in predictive system development and their methodologies.
